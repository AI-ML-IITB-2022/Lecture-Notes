\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{scribe}
\usepackage{listings}
\usepackage{romannum}


\Scribe{}
\Lecturer{Abir De}
\LectureNumber{14}
\LectureDate{29th Sept 2022}
\LectureTitle{Kernel Methods - \Romannum{2}}

\lstset{style=mystyle}

\begin{document}

\MakeScribeTop

\section{Kernel for probability distributions}
In the previous lecture we had seen that a Kernel can be defined as an inner product in the feature space as thus:
\begin{equation}
    K(x, x') = \langle \phi(x),\phi(x') \rangle
\end{equation}
Extending on this definition let $x \in A, x' \in B$, and $x,$ $x'$ be drawn from probability distributions $P_1, P_2$ respectively. We can define a Kernel K over the sets A,B as such:
\begin{align}
    K(A,B) &= \int_{-\infty}^{\infty} \phi_A^T(x).\phi_B(x')Pr(x,x')\\
    &= \iint_{x \in A, x' \in B} \phi^T(x)\phi(x')dP(x,x')
\end{align}
Now, one possible measure of similitarity between the two sets A and B is $P(A \cap B) - P(A)P(B)$. To show that this indeed is a valid kernel, we can show that there exists some $\phi$ such that $K(A,B) = P(A \cap B) - P(A)P(B)$ and $K(A,B)$ satisfies Equation (3).
\\

\noindent Consider $\phi_A(x) = \mathbb{I}_A(x) - P(A)$ where $\mathbb{I}_A(x)$ is the indicator function:

\begin{equation*}
    \mathbb{I}_A(x) = 
    \begin{cases}
      1 & x \in A \\
      0 & \text{otherwise}
    \end{cases}
\end{equation*}
\\
Substituting in Equation (2) we get:
\begin{align*}
    K(A,B) &= \int_{-\infty}^{\infty} (\mathbb{I}_A(x) - P(A))(\mathbb{I}_B(x') - P(B))Pr(x,x')\\
    &= \int_{-\infty}^{\infty} (\mathbb{I}_A(x) \mathbb{I}_B(x')) Pr(x,x') - P(A)\int_{-\infty}^{\infty}  (\mathbb{I}_B(x')) Pr(x,x') \\
    &- P(B)\int_{-\infty}^{\infty}  (\mathbb{I}_A(x')) Pr(x,x') + P(A)P(B)\int_{-\infty}^{\infty} Pr(x,x')\\
    &= P(A \cap B) - P(A)P(B) - P(A)P(B) + P(A)P(B)\\
    &= P(A \cap B) - P(A)P(B)
\end{align*}


\section{Inner Product of Functions}
We define inner product of two functions f,g as: 
\[
    \langle f,g\rangle \doteq \int_{x,y}f(x)g(y)dP(x,y)
\]
Under this inner product definition, K is Kernel for $\phi$. \\
Properties of inner product: 
\begin{enumerate}
    \item Positive Semidefinite: $\langle f,f\rangle\;\geq 0$
    \item Symmetric: $\langle f,g\rangle = \langle g,f\rangle$
    \item Linearity: $<\langle c_{1}f_{1} + c_{2}f_{2},g\rangle = c_{1}\langle f_{1},g\rangle + c_{2}\langle f_{2},g\rangle$
\end{enumerate}
We define norm of function using this inner product definition as:
\[
    \lVert f\rVert \doteq \sqrt{\langle f,f\rangle}
\]

\section{Finding similarity from loss}
Until now, we assumed that Kernel was given to us and we used the kernel as a measure of similarity. But what if we have to find the Kernel if the loss is given.\\
Let F(w) be defined as follows:\\
\begin{equation*}
    F(w) = \sum_{i \in D} l( h_w(x_i), y_i)
\end{equation*}
Given this loss, we now need to find the similarity between points in Dataset.
\begin{center}
\[
\begin{bmatrix}
Loss(x_{i} | t=0)\\
Loss(x_{i} | t=1)\\
.\\
.\\
.\\
\end{bmatrix}
  \]  
\end{center}
We make vectors of all $x_{i}$'s and compare their similarity. 
We use this to select batches of data which have different training curve.
If loss of two points is similar, we can not say that the points themselves are also similar. This is because weights are randomly initialized, so loss cannot be a good measure of similarity. We can instead use gradient of loss to find similarity.
\\
If $X_i \sim X_j$ then $\nabla_w l(h_w(x_i),y_i) \approx \nabla_w l(h_w(x_j),y_j)$ but not the other way round.\\
Therefore, we can define the kernel as follows:
\begin{equation*}
    K(x_i,x_j) = E_w[\nabla_w^{T} l(h_w(x_i),y_i) . \nabla_w l(h_w(x_j),y_j)]
\end{equation*}

\section{Final Problem}
Consider now the following optimization objective: 
\begin{equation*}
    \min_{f\in\Lambda}\sum_{i\in\calD}\left(y_i - f(x_i)\right)^2 + \lambda\sum_{i\in\calD}f(x_i)^2
\end{equation*}
where $f$ is defined in the vector space $\Lambda$ of functions generated by the set 
\begin{equation*}
    \left\{k(x_i,\cdot)\right\}_{i\in\calD}
\end{equation*}
which is a linear subspace of $\R^{\calX}$, where $x_i\in\calX$ for all $i\in\calD$ and $k(\cdot,\cdot)$ is the kernel function defined $\calX\times\calX\stackrel{k}{\longrightarrow}\R$. This vector space is also equipped with the following inner product: 
\begin{equation*}
    \left\langle\sum_{i\in\calD}\alpha_ik(x_i,\cdot), \sum_{j\in\calD}\beta_jk(x_j,\cdot)\right\rangle = \sum_{i\in\calD}\sum_{j\in\calD}\alpha_i\beta_jk(x_i,x_j)
\end{equation*}
That the above is an inner product space is easily verified. Indeed, for all $g\in\Lambda$, there are $\alpha_i\in\R$ for all $i\in\calD$ such that $g = \sum_{i\in\calD}\alpha_ik(x_i,\cdot)$.
\begin{equation*}
    \langle g,g\rangle = \sum_{i\in\calD}\sum_{j\in\calD}\alpha_i\alpha_jk(x_i,x_j)\ge0
\end{equation*}
since $k(\cdot,\cdot)$ is a kernel and therefore is positive semidefinite. Linearity in both operands of $\langle\cdot,\cdot\rangle$ is implicit from the definition and finally symmetry of $\langle\cdot,\cdot\rangle$ follows from the symmetry of $k(\cdot,\cdot)$.

As a result, we may rephrase the objective as
\begin{equation*}
    \min_{\mathbf{\alpha}\in\R^{|\calD|}}\sum_{i\in\calD}\left(y_i - \sum_{j\in\calD}\alpha_jk(x_j, x_i)\right)^2 + \lambda\sum_{i\in\calD}\left(\sum_{j\in\calD}\alpha_jk(x_j,x_i)\right)^2
\end{equation*}

We have 
\begin{align*}
    \sum_{i\in\calD}f(x_i)^2 &= \sum_{i\in\calD}\left(\sum_{j\in\calD}\alpha_jk(x_j,x_i)\right)^2\\
    &= \sum_{i\in\calD}\left(\sum_{j\in\calD}\sum_{k\in\calD}\alpha_j\alpha_kk(x_j, x_i)k(x_k,x_i)\right)
\end{align*}

Finally, we note that the norm of $f$ in the aforementioned inner product space is given by 
\begin{align*}
    \left\langle\sum_{i\in\calD}\alpha_ik(x_i,\cdot),\sum_{i\in\calD}\alpha_jk(x_j,\cdot)\right\rangle &= \sum_{i\in\calD}\sum_{j\in\calD}\alpha_i\alpha_jk(x_i,x_j)\\
    &= \mathbf{\alpha}^TG\mathbf{\alpha}
\end{align*}
where $G = \Bigl[k(x_i,x_j)\Bigr]_{|\calD|\times |\calD|}$
and $\alpha = \begin{bmatrix}\alpha_1 & \cdots & \alpha_{|\calD|}\end{bmatrix}^T$.
\section{Homework Problem}
% TODO: Homework Problem
\noindent\textbf{Problem.} Show that the following kernel is positive semidefinite:
\begin{equation*}
    k(\x,\y) = \exp\left(-\frac{\|\x - \y\|^2}{2\sigma^2}\right)
\end{equation*}

\noindent\textbf{Solution.} Note the following equality:
\begin{equation*}
    \int_{-\infty}^{\infty}\frac{1}{\sigma\sqrt{\pi}}\exp\left(-\frac{(x - z)^2}{\sigma^2}\right)\frac{1}{\sigma\sqrt{\pi}}\exp\left(-\frac{(y - z)^2}{\sigma^2}\right)~dz = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x - y)^2}{2\sigma^2}\right)
\end{equation*}

which is equivalent to the following (assuming a Euclidean norm):
\begin{equation*}
    \int_{\R^n} \left(\frac{1}{\sigma}\sqrt{\frac{2}{\pi}}\right)^n\exp\left(-\frac{\|\x - \z\|^2}{\sigma^2}\right)\exp\left(-\frac{\|\y - \z\|^2}{\sigma^2}\right)~d\z = \exp\left(-\frac{\|\x - \y\|^2}{2\sigma^2}\right)
\end{equation*}

Let $\{\x_i\}_{i\in\calD}$ be a set of data points. Then, for any sequence of real numbers $\{c_i\}_{i\in\calD}$, we have 
\begin{align*}
    &\sum_{i\in\calD}\sum_{j\in\calD}c_ic_j\exp\left(-\frac{\|\x_i-\x_j\|^2}{2\sigma^2}\right) \\
    &= \left(\frac{1}{\sigma}\sqrt{\frac{2}{\pi}}\right)^n\int_{\R^n}\sum_{i,j\in\calD\times\calD}c_ic_j\exp\left(-\frac{\|\x_i - \z\|^2}{\sigma^2}\right)\exp\left(-\frac{\|\x_j - \z\|^2}{\sigma^2}\right)~d\mathbf{z}\\
    &= \left(\frac{1}{\sigma}\sqrt{\frac{2}{\pi}}\right)^n\int_{\R^n}\left[\sum_{i\in\calD}c_i\exp\left(-\frac{\|\x_i - \z\|^2}{\sigma^2}\right)\right]^2~d\mathbf{z}\\
    &\ge 0
\end{align*}
which is obviously non-negative. This completes the proof. $\blacksquare$
\end{document}
