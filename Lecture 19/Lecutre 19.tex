\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[fleqn]{amsmath}
\usepackage{scribe}
\Scribe{Course Team}
\Lecturer{Abir De}
\LectureNumber{19}
\LectureDate{20/10/2022}
\LectureTitle{Mixture Models}

\lstset{style=mystyle}

\begin{document}
	\MakeScribeTop

%#############################################################
%#############################################################
%#############################################################
%#############################################################

\section{Introduction}

We observe a data set $D = \{X_i\}_{i=1}^{N}$, where each $X_i = x_i$ is being sampled from one of the K mixture components.

Each of the mixture component is a multivariate Gaussian density with its own parameters $\theta_k = \{\mu_k, \sum_k\}$ 
\begin{center}
$p_k(x_i|\theta_k) = \frac{1}{(2\pi)^{d/2}|\sum_k|^{1/2}} \e^{-\frac{1}{2}(x-\mu_k)^t\sum_k^{-1}(x-\mu_k)}$
\end{center} 
\\

We now have to estimate the parameters of the K mixture components, $\theta_k$ and the mixture weights, which represent the probability that a randomly selected $\bar{x}$ was generated by $k^{th}$ component, $\pi_k = P(c(\bar{x}) = k)$, where $\sum_{k=1}^K \pi_k = 1$.

\section{Computing posterior distribution $P(c(X_i)=k | X_i)$}

Using initial estimates for $\omega$, we obtain the posterior in the following way -
\begin{gather*}
\begin{aligned}
    & P(c(X_i)=k \mid X_{i}, \omega) = \dfrac {P(\bm{\X_i} \mid c(X_i)=k, \theta_k). P(c = k)} {\sum_{m}P(\bm{X_i} \mid c(X_i) = m, \theta_m)P(c = m)} = \dfrac {N(X_i; \theta_k) \pi_k} {\sum_{m}N(X_i; \theta_m) \pi_m}\\
\end{aligned}
\end{gather*}

This follows a direct application of Bayes rule. These membership weights reflect the uncertainty, given $X_i = x_i$ and $\omega$, about which of the K components generated vector $X_i = x_i$.

\section{Maximum Likelihood Estimation}
The complete set of parameters for a mixture model with K components is - 
\begin{center}
    $\omega = \{ \pi_1, \pi_2, .. \pi_K, \theta_1, .., \theta_K \}$
\end{center}
We now maximize the likelihood of data, $P(D) = P(X_1 = x_1, X_2 = x_2, ..., X_N = x_N)$ w.r.t $\omega$.

\begin{gather*} 
\begin{aligned}
    & P(D) =  \prod_{i=1}^{N} P(\bm{\X_i = \x_{i}}) \\ 
    & \implies \log(P(D)) = \sum_{i=1}^{N} \log(P(\bm{\X_i = \x_{i}})) \\ 
\end{aligned}
\end{gather*}

We know that marginal probability of $X_i$ is,

\begin{gather*} 
\begin{aligned}
    & P(\X_i = \x_i) = \sum_{k=1}^{K} P(\bm{\X_{i} = \x_i} \mid c(X_{i}) = k) P(c = k) \\
    & \implies P(\X_i = \x_i) = \sum_{k=1}^{K} P(\bm{\X_{i} = \x_i} \mid c(X_{i}) = k) \pi_k \\
\end{aligned}
\end{gather*}

Using the above,

\begin{gather*} 
\begin{aligned}
    & \log(P(D)) = \sum_{i=1}^{N} \log(\sum_{k=1}^{K} P(\bm{\X_{i}} \mid c(X_{i}) = k) \pi_k) \\
\end{aligned}
\end{gather*}

Differentiating the above w.r.t $\pi_k$, $\mu_k$ and $\sum_k$, we obtain the new parameters (and using the equation presented in Section 2)- 

\begin{center}
    Let $N_k = \sum_{i=1}^N P(c(X_i) = k| X_i, \omega)$ \\ 
      \\
\end{center}
\begin{center}
$\pi_k^{new} = \frac{N_k}{N}$ \\ 
\end{center}
\begin{center}      \\
    $\mu_k^{new} = (\frac{1}{ N_k}) \sum_{i=1}^N X_i. P(c(X_i) = k| X_i, \omega)$ \\ 
    
      \\
\end{center}  
\begin{center}
    $\sum_k^{new} = (\frac{1}{ N_k}) \sum_{i=1}^N P(c(X_i) = k| X_i, \omega). (X_i - \mu_k^{new}). (X_i - \mu_k^{new})^t$
\end{center}
\section{Iterative Procedure for Parameter Estimation}

We now work on choosing a suitable initial prior for $\pi_k$. 
Entropy of a distribution is defined as $-\sum_{i=1}^{N} P(\bm{\X_{i}}) * \log(P(\bm{\X_{i}})) $ where $\bm{\X_{i}}$ are random variables of the distribution. In a K-means cluster distribution, we have $\pi_1, \pi_2, .. , \pi_K$. In order to maximize the randomness, we assign each one of the random variables probability 1/K. \\

Now, using the above initial prior for $\pi_k$, and some initial parameter estimates $\theta_k$, we derive the posterior $P(c(X_i) = k | X_i)$ (membership weights) as presented in Section 2. \\

Using these new membership weights, we calculate the new $\pi_k$, $\mu_k$ and $\sum_k$ using the equations given at the end of Section 3 (derived by differentiating the log likelihood). \\

Using these new parameter estimates, we calculate the new membership weights and repeat the steps again until the value of likelihood of data converges.

\begin{gather*}
\begin{aligned}
    & \text{Log likelihood of data - } \log\prod_{i=1}^{N} P(\bm{\X_{i}}) =  \sum_{i=1}^{N} \log(\sum_{k=1}^{K} P(\bm{\X_{i}} \mid c(\bm{\X_{i}}) = k) P(c = k)) \\
    & \text{Let } P_\omega = P(\bm{\X_{i}} \mid c(\bm{\X_{i}}) = k), P_c = P(c=k \mid \bm{\X_{i}})\\
    & \omega = \omega^{t-1}\\
    & \text{At time t,  } \max_{\omega}  \sum_{i=1}^{N} \log(\sum_{k=1}^{K} P_\omega P_c(\omega^{t-1})) \text{ will give us the new parameter estimates  } \omega
\end{aligned}
\end{gather*}


\section{Representation in terms of Expectation}
We can also represent the likelihood of data $\{\prod_{i=1}^{N} P(\bm{\X_{i}})\}$ as below.

\begin{gather*}
\begin{aligned}
    & \text{Now, }P(\bm{X}) = \sum_Z{P(X|Z) P(Z)} \\ 
    & \text{implies } P(\bm{X}) = \E_{\bm{Z}}[P(\bm{X|Z})] \\ \\ \\
    & \text{Hence, } P(\bm{\X_{i}}) = \E_{c}[P(\bm{\X_i} \mid c)] \\
    & \prod_{i=1}^{N} P(\bm{\X_{i}}) = \prod_{i=1}^{N} \E_{c} [P(\bm{\X_i} \mid c)] \\ \\ \\
    & \text{Now, } \prod_{i=1}^{N} \sum_{k=1}^{K} P(\bm{\X_{i} | c = k}) P(\bm{c = k}) \\
    & \text{is equal to, } \sum_{k_1=1}^{K} \sum_{k_2=1}^{K} \sum_{k_3=1}^{K} .. \sum_{k_N=1}^{K} (\prod_{i=1}^{N} P(\bm{\X_{i} | c = k_i}) P(\bm{c = k_i})) \\
    % & \text{Thus switching product of sum with sum of product, } \\
    & \prod_{i=1}^{N} P(\bm{\X_{i}}) = \E_{(k_1, k_2, k_3..., k_N)}[\prod_{i=1}^{N} P(\bm{\X_i} \mid c = k_i)] \\ \\ \\
\end{aligned}
\end{gather*}

\section{Mixture Model to K-Means iterative algorithm}
 
Entropy of a distribution is defined as $S(X) = \sum_{i=1}^{N} P(X_{i})\log(P(X_{i})) $ where $X_{i}$ are random variables of the distribution. Entropy is maximised when all of these probabilities are equal (easily proved with differentiation). 
\\
\\So we set the prior $w_k= 1/K$ for all k initially to maximise entropy in K-Means. We set random initial parameter estimates $\theta$. In addition, for later iterations we set $P(c=k) = \I(c=k)$ where $\I$ is the delta function.

Using maximum likelihood estimation of data, we calculate the new parameters and weights and stop when the likelihood converges.

%%%%%%%%%%% end of doc
\end{document}
